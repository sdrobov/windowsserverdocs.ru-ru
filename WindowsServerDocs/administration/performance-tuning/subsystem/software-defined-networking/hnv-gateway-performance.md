---
title: Настройка производительности шлюза HNV в программно определенных сетях
description: Рекомендации по настройке производительности шлюза HNV для программно определяемых сетей
ms.prod: windows-server
ms.technology: performance-tuning-guide
ms.topic: article
ms.author: grcusanz; AnPaul
author: phstee
ms.date: 10/16/2017
ms.openlocfilehash: 907b160b143af18a8ede3a9a7975fa8b22753118
ms.sourcegitcommit: 6aff3d88ff22ea141a6ea6572a5ad8dd6321f199
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 09/27/2019
ms.locfileid: "71383505"
---
# <a name="hnv-gateway-performance-tuning-in-software-defined-networks"></a>Настройка производительности шлюза HNV в программно определенных сетях

В этом разделе приведены спецификации оборудования и рекомендации по конфигурации для серверов под управлением Hyper-V и размещения виртуальных машин шлюза Windows Server в дополнение к параметрам конфигурации для виртуальных машин шлюза Windows Server. . Чтобы извлечь лучшую производительность из виртуальных машин шлюза Windows Server, предполагается, что эти рекомендации будут выполнены.
В следующих разделах вы найдете требования к оборудованию и конфигурации для развертывания шлюза Windows Server.
1. Рекомендации по оборудованию для Hyper-V
2. Конфигурация узла Hyper-V
3. Конфигурация виртуальной машины шлюза Windows Server

## <a name="hyper-v-hardware-recommendations"></a>Рекомендации по оборудованию для Hyper-V

Ниже приведена рекомендуемая минимальная конфигурация оборудования для каждого сервера под управлением Windows Server 2016 и Hyper-V.

| Компонент сервера               | Спецификация                                                                                                                                                                                                                                                                   |
|--------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Центральный процессор (ЦП)  | Узлы архитектуры неоднородной памяти (NUMA): 2 <br> Если на узле имеется несколько виртуальных машин шлюза Windows Server, для обеспечения оптимальной производительности каждая виртуальная машина шлюза должна иметь полный доступ к одному узлу NUMA. Он должен отличаться от узла NUMA, используемого физическим адаптером узла. |
| Количество ядер на узел NUMA            | 2                                                                                                                                                                                                                                                                               |
| Технология Hyper-Threading                | Отключено. Технология Hyper-Threading не повышает производительность шлюза Windows Server.                                                                                                                                                                                           |
| ОЗУ     | 48 ГБ                                                                                                                                                                                                                                                                           |
| Сетевые платы | 2 10 ГБ сетевых карт, производительность шлюза зависит от скорости линии. Если ставка линии меньше 10 Гбит/, то номера пропускной способности шлюза также будут отключаться на один и тот же фактор.                                                                                          |

Убедитесь, что количество виртуальных процессоров, назначенных виртуальной машине шлюза Windows Server, не превышает количество процессоров на узле NUMA. Например, если на узле NUMA восемь ядер, количество виртуальных процессоров должно быть не больше восьми. Для лучшей производительности это должно быть 8. Чтобы узнать количество узлов NUMA и количество ядер на узел NUMA, запустите следующий сценарий Windows PowerShell на каждом узле Hyper-V.

```PowerShell
$nodes = [object[]] $(gwmi –Namespace root\virtualization\v2 -Class MSVM_NumaNode)
$cores = ($nodes | Measure-Object NumberOfProcessorCores -sum).Sum
$lps = ($nodes | Measure-Object NumberOfLogicalProcessors -sum).Sum


Write-Host "Number of NUMA Nodes: ", $nodes.count
Write-Host ("Total Number of Cores: ", $cores)
Write-Host ("Total Number of Logical Processors: ", $lps)
```

>[!Important]
> Выделение виртуальных процессоров за пределами узлов NUMA может отрицательно сказаться на производительности шлюза Windows Server. Использование нескольких виртуальных машин, виртуальные процессоры каждой из которых относятся к одному узлу NUMA, обеспечивает более высокую производительность, чем использование одной виртуальной машины, которой назначены все виртуальные процессоры.

При выборе количества виртуальных машин шлюза, устанавливаемых на каждом узле Hyper-V, если на каждом узле NUMA восемь ядер, рекомендуется использовать одну виртуальную машину шлюза с восемью виртуальными процессорами и по крайней мере 8 ГБ ОЗУ. В этом случае один узел NUMA выделяется для хост-компьютера.

## <a name="hyper-v-host-configuration"></a>Конфигурация узла Hyper-V

Ниже приведена рекомендуемая конфигурация для каждого сервера под управлением Windows Server 2016 и Hyper-V, Рабочая нагрузка которого заключается в запуске виртуальных машин шлюза Windows Server. Эти инструкции по конфигурации включают примеры использования команд Windows PowerShell. В этих примерах вместо актуальных значений, которые вы должны предоставить при выполнении команды, используются заполнители. Например, заполнители имени сетевого адаптера: «NIC1» и «NIC2». Когда вы выполняете команды, в которых используются эти заполнители, используйте фактические имена сетевых адаптеров на ваших серверах, иначе команда не будет выполнена.

>[!Note]
> Для запуска следующих команд Windows PowerShell вы должны быть членом группы администраторов.

| Элемент конфигурации                          | Конфигурация Windows PowerShell                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
|---------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Объединение внедренных коммутаторов                     | При создании vSwitch с несколькими сетевыми адаптерами он автоматически включил объединение внедренных коммутаторов для этих адаптеров. <br> ```New-VMSwitch -Name TeamedvSwitch -NetAdapterName "NIC 1","NIC 2"``` <br> Традиционная поддержка объединения через LBFO не поддерживается с использованием SDN в Windows Server 2016. Включение поддержки объединения внедренных элементов позволяет использовать один и тот же набор сетевых карт для виртуального трафика и для трафика RDMA. Это не поддерживалось объединением сетевых карт на основе LBFO.                                                        |
| Управление прерываниями на физических сетевых адаптерах       | Используйте параметры по умолчанию. Чтобы проверить конфигурацию, можно использовать следующую команду Windows PowerShell: ```Get-NetAdapterAdvancedProperty```.                                                                                                                                                                                                                                                                                                                                                                    |
| Размер буферов приема физических сетевых адаптеров       | Чтобы проверить, поддерживает ли физические сетевые карты конфигурацию этого параметра, выполнив команду ```Get-NetAdapterAdvancedProperty```. Если они не поддерживают этот параметр, то выходные данные команды не включают свойство «буферы получения». Если сетевые адаптеры поддерживают этот параметр, можно воспользоваться следующей командой Windows PowerShell, чтобы настроить размер буферов приема. <br>```Set-NetAdapterAdvancedProperty "NIC1" –DisplayName "Receive Buffers" –DisplayValue 3000``` <br>                          |
| Размер буферов отправки физических сетевых адаптеров          | Чтобы проверить, поддерживает ли физические сетевые карты конфигурацию этого параметра, выполнив команду ```Get-NetAdapterAdvancedProperty```. Если сетевые адаптеры не поддерживают этот параметр, выходные данные команды не включают в себя свойство «отправка буферов». Если сетевые адаптеры поддерживают этот параметр, можно воспользоваться следующей командой Windows PowerShell, чтобы настроить размер буферов отправки. <br> ```Set-NetAdapterAdvancedProperty "NIC1" –DisplayName "Transmit Buffers" –DisplayValue 3000``` <br>                           |
| Масштабирование на стороне приема на физических сетевых адаптерах | Чтобы проверить, включены ли RSS-канал для физических сетевых адаптеров, запустите команду Windows PowerShell Get-Нетадаптеррсс. Для включения и настройки RSS-канала на сетевых адаптерах можно использовать следующие команды Windows PowerShell: <br> ```Enable-NetAdapterRss "NIC1","NIC2"```<br> ```Set-NetAdapterRss "NIC1","NIC2" –NumberOfReceiveQueues 16 -MaxProcessors``` <br> ПРИМЕЧАНИЕ. Если ВММК или VMQ включены, то на физических сетевых адаптерах не нужно включать поддержку RSS. Его можно включить на виртуальных сетевых адаптерах узла. |
| ВММК                                        | Чтобы включить ВММК для виртуальной машины, выполните следующую команду: <br> ```Set-VmNetworkAdapter -VMName <gateway vm name>,-VrssEnabled $true -VmmqEnabled $true``` <br> ПРИМЕЧАНИЕ. Не все сетевые адаптеры поддерживают ВММК. В настоящее время она поддерживается для серии Chelsio T5 и T6, Mellanox CX-3 и CX-4 и QLogic 45xxx.                                                                                                                                                                                                                                      |
| Очередь виртуальной машины в объединении сетевых карт | Вы можете включить VMQ в команде SET с помощью следующей команды Windows PowerShell: <br>```Enable-NetAdapterVmq``` <br> ПРИМЕЧАНИЕ. Эта возможность должна быть включена, только если оборудование не поддерживает ВММК. Если поддерживается, ВММК должен быть включен для лучшей производительности.                                                                                                                                                                                                                                                               |
>[!Note]
> VMQ и vRSS поступают в изображение только в том случае, если нагрузка на виртуальную машину высока и максимальная загруженность ЦП. В этом случае будет вычислено по крайней мере одно ядро процессора. VMQ и vRSS помогут распределить нагрузку на несколько ядер. Это неприменимо для трафика IPsec, так как трафик IPsec ограничивается одним ядром.

## <a name="windows-server-gateway-vm-configuration"></a>Конфигурация виртуальной машины шлюза Windows Server

На обоих узлах Hyper-V можно настроить несколько виртуальных машин, которые настроены в качестве шлюзов с помощью шлюза Windows Server. Можно воспользоваться диспетчером виртуального коммутатора, чтобы создать виртуальный коммутатор Hyper-V, привязанный к объединению сетевых карт на узле Hyper-V. Обратите внимание, что для лучшей производительности следует развернуть одну виртуальную машину шлюза на узле Hyper-V.
Ниже представлена рекомендованная конфигурация для виртуальной машины шлюза Windows Server.

| Элемент конфигурации                 | Конфигурация Windows PowerShell                                                                                                                                                                                                                                                                                                                                                               |
|------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Память                             | 8 ГБ                                                                                                                                                                                                                                                                                                                                                                                           |
| Количество виртуальных сетевых адаптеров | 3 сетевые карты со следующими конкретными применениями: 1 для управления, которое используется операционной системой управления, 1 Внешняя, обеспечивающая доступ к внешним сетям, 1 — внутренний, обеспечивающий доступ только к внутренним сетям.                                                                                                                                                            |
| Receive Side Scaling (RSS)         | Вы можете удержать параметры RSS по умолчанию для сетевого адаптера управления. Ниже приведен пример конфигурации для виртуальной машины с восемью виртуальными процессорами. Для внешних и внутренних сетевых адаптеров можно включить RSS с параметром Басепрокнумбер, равным 0, а Максрсспроцессорс — 8 с помощью следующей команды Windows PowerShell: <br> ```Set-NetAdapterRss "Internal","External" –BaseProcNumber 0 –MaxProcessorNumber 8``` <br> |
| Отправить буфер на стороне                   | Можно удержать параметры буфера отправки по умолчанию для сетевого адаптера управления. Как для внутренних, так и для внешних сетевых адаптеров можно настроить буфер отправки с 32 МБ ОЗУ с помощью следующей команды Windows PowerShell: <br> ```Set-NetAdapterAdvancedProperty "Internal","External" –DisplayName "Send Buffer Size" –DisplayValue "32MB"``` <br>                                                       |
| Буфер приема                | Вы можете удержать параметры буфера приема по умолчанию для сетевого адаптера управления. Для внутренних и внешних сетевых адаптеров можно настроить буфер приема размером 16 МБ ОЗУ с помощью следующей команды Windows PowerShell: <br> ```Set-NetAdapterAdvancedProperty "Internal","External" –DisplayName "Receive Buffer Size" –DisplayValue "16MB"``` <br>                                            |
| Оптимизация переадресации               | Для сетевого адаптера управления можно использовать параметры по умолчанию для оптимизации вперед. Для внутренних и внешних сетевых адаптеров можно включить прямую оптимизацию с помощью следующей команды Windows PowerShell: <br> ```Set-NetAdapterAdvancedProperty "Internal","External" –DisplayName "Forward Optimization" –DisplayValue "1"``` <br>                                                                      |
